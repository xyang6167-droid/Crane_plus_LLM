"""
CranePlus æ¨¡å‹äº¤äº’å¼å¯¹è¯è„šæœ¬ï¼ˆç®€åŒ–ç‰ˆï¼‰
"""
import argparse
import torch
from transformers import AutoTokenizer, TextStreamer
from model.config import Crane_PlusConfig
from model.model_crane_plus import CraneForCausalLM

def main():
    parser = argparse.ArgumentParser(description="SpongeBobæ¨¡å‹äº¤äº’å¯¹è¯")
    parser.add_argument('--model_path', default='', type=str, help="æ¨¡å‹æƒé‡è·¯å¾„ï¼ˆ.pthæ–‡ä»¶ï¼‰")
    parser.add_argument('--tokenizer_path', default='./tokenizer_15k', type=str, help="Tokenizerè·¯å¾„")
    parser.add_argument('--model_type', default='sft', type=str, choices=['pretrain', 'sft'], help="æ¨¡å‹ç±»å‹ï¼špretrainï¼ˆæ–‡æœ¬ç»­å†™ï¼‰æˆ– sftï¼ˆå¯¹è¯ï¼‰")
    parser.add_argument('--hidden_size', default=768, type=int, help="éšè—å±‚ç»´åº¦")
    parser.add_argument('--num_hidden_layers', default=12, type=int, help="éšè—å±‚æ•°é‡")
    parser.add_argument('--max_new_tokens', default=2048, type=int, help="æœ€å¤§ç”Ÿæˆé•¿åº¦")
    parser.add_argument('--temperature', default=0.2, type=float, help="ç”Ÿæˆæ¸©åº¦ï¼ˆ0-1ï¼‰")
    parser.add_argument('--top_p', default=0.7, type=float, help="nucleusé‡‡æ ·é˜ˆå€¼")
    parser.add_argument('--device', default='cuda' if torch.cuda.is_available() else 'cpu', type=str)
    parser.add_argument('--multi_turn', action='store_true', help="ä¿ç•™å¯¹è¯å†å²ï¼ˆå¤šè½®ï¼‰ï¼›ä¸ä¼ åˆ™å•è½®ï¼Œæ¯è½®ç‹¬ç«‹")
    args = parser.parse_args()
    
    # è‡ªåŠ¨æ¨æ–­æ¨¡å‹ç±»å‹ï¼ˆä»æ–‡ä»¶åï¼‰
    if 'pretrain' in args.model_path:
        args.model_type = 'pretrain'
    elif 'sft' in args.model_path:
        args.model_type = 'sft'
    
    # åŠ è½½æ¨¡å‹å’Œtokenizer
    print(f'åŠ è½½æ¨¡å‹: {args.model_path}')
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path)
    
    model = CraneForCausalLM(Crane_PlusConfig(
        hidden_size=args.hidden_size,
        num_hidden_layers=args.num_hidden_layers
    ))
    model.load_state_dict(torch.load(args.model_path, map_location=args.device))
    model.eval().to(args.device)
    
    print(f'âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼è®¾å¤‡: {args.device}')
    print(f'ğŸ“ æ¨¡å‹ç±»å‹: {args.model_type} ({"å¯¹è¯æ¨¡å¼" if args.model_type == "sft" else "æ–‡æœ¬ç»­å†™"})')
    print(f'ğŸ“ å¯¹è¯æ¨¡å¼: {"å¤šè½®ï¼ˆä¿ç•™å†å²ï¼‰" if args.multi_turn else "å•è½®ï¼ˆæ¯è½®ç‹¬ç«‹ï¼‰"}\n')
    print('='*60)
    print('ğŸ’¬ å¼€å§‹å¯¹è¯ (è¾“å…¥ exit é€€å‡º)')
    print('='*60)
    
    conversation = []  # ä»… multi_turn æ—¶ä½¿ç”¨
    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=False)
    
    while True:
        user_input = input('\nğŸ‘¤ ä½ : ').strip()
        
        if user_input.lower() in ['exit', 'quit', 'é€€å‡º']:
            print('ğŸ‘‹ å†è§ï¼')
            break
        
        if not user_input:
            continue
        
        if args.model_type == 'pretrain':
            formatted_input = user_input
            conversation = []
        else:
            # SFTï¼šæŒ‰æ˜¯å¦å¤šè½®å†³å®šæ˜¯å¦ä¿ç•™å†å²
            if args.multi_turn:
                conversation.append({"role": "user", "content": user_input})
            else:
                conversation = [{"role": "user", "content": user_input}]
            formatted_input = tokenizer.apply_chat_template(
                conversation=conversation,
                tokenize=False,
                add_generation_prompt=True
            )
        
        inputs = tokenizer(formatted_input, return_tensors="pt").to(args.device)
        
        # ç”Ÿæˆå›å¤
        print('CranePlus: ', end='', flush=True)
        with torch.no_grad():
            generated_ids = model.generate(
                inputs=inputs["input_ids"],
                attention_mask=inputs["attention_mask"],
                max_new_tokens=args.max_new_tokens,
                do_sample=True,
                streamer=streamer,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
                top_p=args.top_p,
                temperature=args.temperature,
                repetition_penalty=1.2
            )
        
        response = tokenizer.decode(
            generated_ids[0][len(inputs["input_ids"][0]):],
            skip_special_tokens=False
        )
        if args.model_type == 'sft' and args.multi_turn:
            conversation.append({"role": "assistant", "content": response})

if __name__ == "__main__":
    main()
